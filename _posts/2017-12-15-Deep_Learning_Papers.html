---
layout: default
title: Deep Learning Papers
---
<h2>{{ page.title }}</h2>

<head>
<style type="text/css">
table {
    display: table;
    /*border-collapse: collpase;*/
    border-spacing: 0px;
    border-color: #A52A2A;
    border: 0.5px solid;
    cell-spacing: 0;
    border-radius:5px;
    font-family: Arial, Helvetica, sans-serif;
}
table, th, td {
    padding:.1cm .1cm .1cm .1cm;
    border: 0.5px solid #A52A2A;
    vertical-align: top;
}
th
{
    bgcolor: grey;
}
a {
    color: blue;
    text-decoration: none;
}
td {
    word-spacing: 2px;
    text-align: left;
}
.paperTitle{
    font-size:23px
    color: blue;
}
</style>
</head>

<!--<table margin-top="15px" border-collapse="collapse" border="1px solid #aaa" width="100%" cellspacing="0.1" cellpadding="20">-->
<!--<table style = "margin-top: 15px; border-collapse: collapse; border: 1px solid #aaa; width: 100%;">-->
<table>
  <tbody>
    <tr>
      <th width="20%">Paper</th>
      <th width="5%">Year</th>
      <th width="15%">Author</th>
      <th width="10%">Keywords</th>
      <th width="45%">Notes</th>
    </tr>
      
     <tr>
      <td class = "paperTitle"><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-bengio-95a.pdf">
                   Convolutional networks for images, speech, and time series</a></td>
      <td>1998</td>
      <td>Yann LeCun, Yoshua Bengio</td>
      <td>convoluational network</td>
      <td></td>
    </tr>
    
    <tr>
      <td class = "paperTitle"><a href="https://arxiv.org/pdf/1311.2524.pdf">
                   Rich feature hierarchies for accurate object detection and semantic segmentation</a></td>
      <td>2013</td>
      <td>Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik</td>
      <td></td>
      <td></td>
    </tr>

     <tr>
      <td class = "paperTitle"><a href="https://research.cs.washington.edu/istc/lfb/paper/cvpr12.pdf">
                   Rgb-(d) scene labeling: Features and algorithms</a></td>
      <td>2012</td>
      <td></td>
      <td>scene labeling</td>
      <td>
          <p>Goal: indoor scene labeling</p>
          <p>Gap:  large variations of scene types, lack of distinctive features, and poor illumination</p>
          <p>Method: combines rich RGB-D features and contextual models using MRFs and hierarchical segmentation.</p>
          <p>Notes: use kernel descriptor(2, 3), aggregate over superpixel </p>
          <p>[RGB-D sensors, MRF (markov random fields, 8, 16, 18, 32), CRF(conditional random fields), 
              <a href="https://infoscience.epfl.ch/record/177415">superpixel</a>, multiple segmentations(6,16,27), 
              hierarchical segmentations(21,23), ]</p>
      </td>
    </tr>
    
    <tr>
      <td class = "paperTitle"><a href="http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf">
                   Learning hierarchical features for scene labeling</a></td>
      <td>2013</td>
      <td>Cle ÃÅment Farabet, Camille Couprie, Laurent Najman, Yann LeCun</td>
      <td>scene labeling</td>
      <td>
          <p>Goal: </p>
      </td>
    </tr>

    <tr>
      <td class = "paperTitle"><a href="https://www.inf.ethz.ch/personal/ladickyl/road_bmvc09.pdf">
                   Combining appearance and structure from motion features for road scene understanding</a></td>
      <td>2013</td>
      <td></td>
      <td></td>
      <td>
          <p>Goal: </p>
      </td>
    </tr>
      
    <tr>
      <td class = "paperTitle"><a href="https://arxiv.org/pdf/1512.03385.pdf">
                   Deep Residual Learning for Image Recognition</a></td>
      <td>2015</td>
      <td>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</td>
      <td>resNet</td>
      <td>
          <p>Goal: </p>
      </td>
    </tr>
      
    <tr>
      <td class = "paperTitle"><a href="https://arxiv.org/pdf/1502.03167.pdf">
                   Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift</a></td>
      <td>2015</td>
      <td>Sergey Ioffe, Christian Szegedy</td>
      <td><a href="http://blog.csdn.net/hjimce/article/details/50866313">
          batch norm</a>(overwhelm whitening, accelearte trainnig speed, overfitting proof)</td>
      <td>
          <p>Goal: </p>
          
      </td>
    </tr>
      
    <tr>
      <td class = "paperTitle"><a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">
                   Fully convolutional networks for semantic segmentation</a></td>
      <td></td>
      <td></td>
      <td></td>
      <td>
          <p>Goal: </p>
          <p>[overfeat(32); <a href="http://blog.csdn.net/hjimce/article/details/50866313">
              batch normalization/spatial batch normalization</a>; ]</p>
          <p>{can improve the dense prediction policy (to beat deconvolution)}</p>
      </td>
    </tr>

    <tr>
      <td class = "paperTitle"><a href="https://arxiv.org/pdf/1606.00915.pdf">
                   Semantic image segmentation with deep convolutional nets and fully connected crfs</a></td>
      <td></td>
      <td></td>
      <td></td>
      <td>
          <p>Goal: </p>
      </td>
    </tr>

    <tr>
      <td class = "paperTitle"><a href="https://arxiv.org/pdf/1312.6229.pdf">
                   Overfeat: Integrated recognition, localization and detection using convolutional networks</a></td>
      <td></td>
      <td></td>
      <td>fully-convolutional network</td>
      <td>
          <p>Goal: </p>
      </td>
    </tr>

    <tr>
      <td class = "paperTitle"><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf">
                   Backpropagation applied to hand-written zip code recognition</a></td>
      <td>1989</td>
      <td></td>
      <td><p>leNet</p></td>
      <td>
          <p>Goal: Handwritten digits recogonition</p>
          <p>Gap:</p>
          <p>Method: extract local features and combine them into higher order feature; introduce feature detector via "weight sharing";
          feature map (one of several planes in a conv hidden layer, all unit in the plane share a same set of weight);
          bias (threshold) not shared;</p>
      </td>
    </tr>

    <tr>
      <td class = "paperTitle"><a href="https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf">
                   Imagenet classification with deep convolutional neural networks</a></td>
      <td></td>
      <td></td>
      <td>AlexNet; ReLu; local-response normalization; fully-convolutional network</td>
      <td>
          <p>Goal: image classification on realistic objects</p>
          <p>Gap: learn large number of objects</p>
          <p>Method: convolutional neural network (flexible in network layer width and dept, much fewer parameters, comparable performance to 
              fully connected network); introduce GPU to learn</p>
          <p>{should encode info into nn, not just use tons of weights to (simulate large number of dimensions of borders) filter/classify 
              objects, that would improve the real intelligence of network}</p>
          <p>[ReLu nonliearity; <a href="https://www.zhihu.com/question/24529483">momentum variable</a>; white decap]</p>
      </td>
    </tr>

    <tr>
      <td class = "paperTitle"><a href="https://arxiv.org/pdf/1409.1556.pdf">
                   Very deep convolutional networks for large-scale image recognition</a></td>
      <td>2015</td>
      <td></td>
      <td></td>
      <td>
          <p>Goal: evaluation image recognition accuracy along network depth varys</p>
          <p>Gap:</p>
          <p>Method:smaller receptive field; smaller stride size; test stacked conv layers with increasing layer number;
          pre-training on network weight initialization (or alternative method stated in "Understanding the difficulty of 
              training deep feedforward neural networks"); single-scale/multi-scale/multi-crop evaluation;</p>
          <p>{adjust receptive field size, stride size; train with multi-scale of origin image; deeper network can apply smaller size of
              receptive field (e.g., a 9x9 layer can in certain level be viewed as 3 stacked 3x3 conv layers)}</p>
          <p>[argue that local response normalization is in-effective]</p>
      </td>
    </tr>

    <tr>
      <td class = "paperTitle"><a href="http://www-prima.imag.fr/jlc/Courses/2016/PRML/XavierInitialisation.pdf">
                   Understanding the difficulty of training deep feedforward neural networks</a></td>
      <td>2010</td>
      <td></td>
      <td>weight initialization</td>
      <td>
          <p>Goal:</p>
          <p>Gap:</p>
          <p>Method:</p>
      </td>
    </tr>

    <tr>
      <td class = "paperTitle"><a href="https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf">
                   Going deeper with convolutions</a></td>
      <td>2015</td>
      <td></td>
      <td>GoogLeNet</td>
      <td>
          <p>Goal: </p>
          <p>[ensemble approach; Hebbian principle; translation invariance; 
              <a href="https://www.cnblogs.com/neuface/archive/2016/03/11/5265740.html">inception module</a>; 
              depthConcat(A depth concatenation layer takes multiple inputs that have the same height and width and 
              concatenates them along the third dimension, i.e., the channel dimension);]</p>
      </td>
    </tr>
      
    <tr>
      <td class = "paperTitle"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.638.2525&rep=rep1&type=pdf">
                   Scaling up matrix computations on shared-memory manycore systems with 1000 cpu cores</a></td>
      <td>2014</td>
      <td></td>
      <td></td>
      <td>
          <p>Goal: </p>
          <p></p>
      </td>
    </tr>
      
    <tr>
      <td class = "paperTitle"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.310.4409&rep=rep1&type=pdf">
                   On two-dimensional sparse matrix partitioning: Models, methods, and a recipe</a></td>
      <td>2010</td>
      <td></td>
      <td></td>
      <td>
          <p>Goal: </p>
          <p></p>
      </td>
    </tr>

    <tr>
      <td class = "paperTitle"><a href="https://arxiv.org/pdf/1312.4400.pdf">
                   Network in network</a></td>
      <td>2014</td>
      <td></td>
      <td></td>
      <td>
          <p>Goal: </p>
          <p></p>
      </td>
    </tr>

  </tbody>
</table>

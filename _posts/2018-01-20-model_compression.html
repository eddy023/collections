---
layout: default
title: Model Compression and Acceleration Papers
---
<h2>{{ page.title }}</h2>

<h3>A Survey of Model Compression and Acceleration for Deep Neural Networks</h3>
<p><B>Author:</B></p>
<p><B>Year:</B></p>
<p>
<B>Goal:</B>
</br>
<B>Gap:</B>
</br>
<B>Method:</B>
</br>

</p>

<h3>Deep Compression - Compressing Deep Nerual Networks with Pruning, Trained Quantization and Huffman Codeing</h3>
<p><B>Author</B>:Song Hanï¼Œ Huizi Mao, William J. Dally</p>
<p><B>Year</B>:2016</p>
<p>
<B>Goal</B>: save memory to fit mobile device, accelerate computation speed
</br>
<B>Gap</B>: enormous parameters for dl models
</br>
<B>Method</B>: 3-stage strategy: prunning less-important weights, quatization weights and encode weigths with Huffman coding
</br>
<B>Notes</B>:
</br>
["Learning both weights and connections for efficient neural networks", 2015 work prune weights without accuracy loss] 
</br>
['compressed sparse row/column format']
</br>
</p>

<h3>Learning both weights and connections for efficient neural networks</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:2015</p>
<p>
<B>Goal</B>: save memory to fit mobile device, accelerate computation speed
</br>
<B>Gap</B>: enormous parameters for dl models
</br>
<B>Method</B>: 3-step approach: train to find weight importance, prune less importance weight and re-train for fine tunning
</br>
<B>Notes</B>:
["Network in network", "Going Deeper with Convolutions", replace fully connected layers with global average pooling and add a liner layer
to enable tranfer learning.]
</br>
["Comparing biases for minimal network construction with back-propagation", 1989, biased weight decay]
</br>
["Optimal brain damage", 1990, Yann le Cun]
</br>
["Second order derivatives for network pruning: Optimal brain surgeon", 1993, Babak Hassibi]
</br>
["Compressingneural networks with the hashing trick", hashnet]
</br>
["Hash kernels for structured data", "Feature hashing for large scale multitask learning", sparsity will minimize hash collision]
</br>
[? FLOP]

</br>

</p>

<h3>Comparing biases for minimal network construction with back-propagation</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>:
</br>
<B>Notes</B>:
</br>
[? colliearity: Technically, a set of vectors is collinear when a linear function is exactly equal to zero.]
</br>
[? ridge regression, principle component regression]
</br>
[! https://www.zhihu.com/question/24529483]
</p>

<h3>Improving the speed of neural networks on cpus</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>:
</br>
<B>Notes</B>:
</br>

</p>

<h3>Fast training of convolutional networks through ffts</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>:
</br>
<B>Notes</B>:
</br>

</p>

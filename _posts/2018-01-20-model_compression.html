---
layout: default
title: Model Compression and Acceleration Papers
---
<h2>{{ page.title }}</h2>

<h3>A Survey of Model Compression and Acceleration for Deep Neural Networks</h3>
<p><B>Author:</B></p>
<p><B>Year:</B></p>
<p>
<B>Goal:</B>
</br>
<B>Gap:</B>
</br>
<B>Method:</B>
</br>
<B>Notes:</B>
</br>
[? Discrete cosine transform, wavelets system]
</p>

<h3>Predicting parameters in deep learning</h3>
<p><B>Author:</B></p>
<p><B>Year:</B></p>
<p>
<B>Goal:</B>
</br>
<B>Gap:</B>
</br>
<B>Method:</B>
</br>
<B>Notes</B>:
</br>
[! this paper pointing out: there is a surprisingly large amount of redundancy among the weights of neural networks]
</p>

<h3>Deep Compression - Compressing Deep Nerual Networks with Pruning, Trained Quantization and Huffman Codeing</h3>
<p><B>Author</B>:Song Han， Huizi Mao, William J. Dally</p>
<p><B>Year</B>:2016</p>
<p>
<B>Goal</B>: save memory to fit mobile device, accelerate computation speed
</br>
<B>Gap</B>: enormous parameters for dl models
</br>
<B>Method</B>: 3-stage strategy: prunning less-important weights, quatization weights and encode weigths with Huffman coding
</br>
<B>Notes</B>:
</br>
["Learning both weights and connections for efficient neural networks", 2015 work prune weights without accuracy loss] 
</br>
['compressed sparse row/column format']
[! Compressed Sparse Row (CSR) : 2*nnz + row_size number of memory
</br>
Compressed Sparse Column (CSC) : 2*nnz + column_size number of memory
</br>
Coordinate Format (COO) : 3*nnz number of memory
]
</br>

</p>

<h3>Learning both weights and connections for efficient neural networks</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:2015</p>
<p>
<B>Goal</B>: save memory to fit mobile device, accelerate computation speed
</br>
<B>Gap</B>: enormous parameters for dl models
</br>
<B>Method</B>: 3-step approach: train to find weight importance, prune less importance weight and re-train for fine tunning
</br>
<B>Notes</B>:
["Network in network", "Going Deeper with Convolutions", replace fully connected layers with global average pooling and add a liner layer
to enable tranfer learning.]
</br>
["Comparing biases for minimal network construction with back-propagation", 1989, biased weight decay]
</br>
["Optimal brain damage", 1990, Yann le Cun]
</br>
["Second order derivatives for network pruning: Optimal brain surgeon", 1993, Babak Hassibi]
</br>
["Compressingneural networks with the hashing trick", hashnet]
</br>
["Hash kernels for structured data", "Feature hashing for large scale multitask learning", sparsity will minimize hash collision]
</br>
[? FLOP]

</br>

</p>

<h3>Comparing biases for minimal network construction with back-propagation</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>:
</br>
<B>Notes</B>:
</br>
[? colliearity: Technically, a set of vectors is collinear when a linear function is exactly equal to zero.]
</br>
[? ridge regression, principle component regression]
</br>
[! https://www.zhihu.com/question/24529483]
</p>

<h3>Compressing Deep Convolutional Networks Using Vector Quantization</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>:
</br>
Matrix factorization: Approximation original matrix by SVD's top K eigen-vectors and eigen-values
</br>
Vector Quantization: binarization; scalar quantization with K-means; product quantization; residual quantization
<B>Notes</B>:
</br>
[? Frobenius norm: The SVD method is optimal in the sense of a Frobenius norm, 
which minimizes the MSE error between the approximated matrix Wˆ and original W]
</br>

</p>

<h3>Improving the speed of neural networks on cpus</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>:
</br>
<B>Notes</B>:
</br>

</p>

<h3>Fast training of convolutional networks through ffts</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>:
</br>
<B>Notes</B>:
</br>

</p>

<h3>Exploiting linear structure within convolutional networks for efficient evaluation</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>: Low-rank factorization
</br>
<B>Notes</B>:
</br>
[? Mahalanobis distance]
</br>
[=> http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html]
</p>

<h3>Speeding up convolutional neural networks with low rank expansions</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>: Low-rank factorization
</br>
<B>Notes</B>:
</br>

</p>

<h3>Improving neural networks by preventing co-adaptation of feature detectors</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>:
</br>
<B>Notes</B>:
</br>
[=> redundancy of parameters in nerual network seems necessary in order to overcome a highly non-convex optimization]
</p>

<h3>Distilling the knowledge in a neural network</h3>
<p><B>Author</B>:Geoffrey Hinton, Oriol Vinyals, Jeff Dean</p>
<p><B>Year</B>:2015</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>:
</br>
<B>Notes</B>:
</br>
["Dropout: A simple way to prevent neural networks from overfitting", dropout, a very strong way of regularizer]
[=> softmax, temperature makes softmmax more soft =>summary]
</p>

<h3>Fitnets: Hints for thin deep nets</h3>
<p><B>Author</B>:</p>
<p><B>Year</B>:</p>
<p>
<B>Goal</B>:
</br>
<B>Gap</B>:
</br>
<B>Method</B>:
</br>
<B>Notes</B>:
</br>
</p>

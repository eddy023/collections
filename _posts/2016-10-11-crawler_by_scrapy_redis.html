---
layout: default
title: Crawler by Scrapy-redis and anti-crawl settings
---
<h3>1 Installation and Commands</h3>
<p>Install scrapy: </p>
<p style="color:red">sudo pip3 install scrapy</p>
<p>Install redis:</p>
<p style="color:red">sudo apt-get install redis-server</p>
<p>Start redis server in the terminal: 
<p style="color:red">redis-server</p>
<p>Start a client:</p>
<p style="color:red">redis-cli</p>
<p>Useful commands of redis:</p>
<p style="color:red">keys *       // query all keys</p>
<p style="color:red">type [key]   // query the structure type of the corresponding key</p>
<p style="color:red">flushdb      // clear all records</p>
<p style="color:red">llen [key]   // check record number of a given key stored using list structure; normally data are 
  stored with list</p>
<p>Connect to a remote redis-server: redis-cli -h [ip] -p [port]</p>
<p>For distributed crawling, a pc should first start a client connecting to the remote redis server and then to start 
  scrapying pages.</p>
<p>ps, I use ubuntu 16.04 and python 3.5.</p>

<h3>2 Architecture and Code</h3>
<p>Fig. 2-1 depicts the architecture of scrapy redis, among all components, we commonly only need to write some code 
  within "spiders", "middleware" and "pipeline". </p>
<p style="text-align:center"><img src="/collections/imgs/10-11-crawler-scrapy-redis.png" style="width:400px;height:300px;" /></p>
<p style="text-align:center; font-style:italic">Fig. 2-1 Architecture of Scrapy-redis</p>
<textarea style="color:red" rows="10" cols="80">
# @ZhihuSpider.py
import re
from scrapy_redis.spiders import RedisSpider
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from lxml import etree
from scrapy.selector import Selector

class zhihuSpider(CrawlSpider):
    name = 'ZhihuSpider'
    allowed_domains = ['https://www.zhihu.com', 'https://www.zhihu.com', 'www.zhihu.com']
    start_urls = ['https://www.zhihu.com/question/50526563#answer-44787525']
    
    rules = [
        Rule(LinkExtractor(), callback='parse_directory', follow=True),
    ] 

    xsrf = None

    def parse_directory(self, response):
        if not (re.match("https://www.zhihu.com/question", response.url) or re.match("http://www.zhihu.com/question", response.url)):
            return
        tree = etree.HTML(response.body)
        r = tree.xpath("//div[contains(@class,'zm-editable-content')]/text() | //span[contains(@class,'zm-editable-content')]/text()")
        #print(r)
        return {'records': r}
</textarea>
<p>Above code gives an example of spider to  scrapy "www.zhihu.com". The architecture will request "start_urls" and call "parse_directory"
with response. The return value of "parse_directory" will be stored into redis with the form defined in "items.py". If a website requires
login to access, the function "start_requests" needs to be defined:</p>
<textarea style="color:red" rows="4" cols="80">
   #defined within class ZhihuSpider
      def start_requests(self):
        return [sp.Request("https://www.zhihu.com/#signin", meta = {'cookiejar' : 1, 'dont_redirect': True}, callback = self.request_captcha)]
</textarea>
<p>The callback function "self.request_captcha" is to request the captcha picture as verification code.</p>
<textarea style="color:red" rows="10" cols="80">
   def request_captcha(self, response):
        self.xsrf = Selector(response).xpath('//input[@name="_xsrf"]/@value').extract()[0]
        captcha_url = "http://www.zhihu.com/captcha.gif?r=" + str(int(time.time() * 1000)) + "&type=login"
        yield Request(
            url=captcha_url,
            headers=self.headers_dict,
            meta={
                "cookiejar": response.meta["cookiejar"],
                "X-Xsrftoken": self.xsrf
            },
            callback=self.download_captcha)
  def download_captcha(self, response):
        with open("captcha.gif", "wb") as fp:
            fp.write(response.body)
        os.system('xdg-open captcha.gif')
        print("请输入验证码:\n")
        captcha = input()
        return sp.FormRequest(
            url="https://www.zhihu.com/login/phone_num",
            headers=self.headers_dict,
            formdata={
                "phone_number": "",
                "password": "",
                "X-Xsrftoken": self.xsrf,
                "remember_me": "true",
                "captcha": captcha
            },
            meta={
                "cookiejar": response.meta["cookiejar"],
                'dont_redirect': True,
            },
            callback=self.after_login)
</textarea>
<p>The callback function of "request_captcha" will download captcha picture and prompt the user to input the code and send login 
request. The "after_login" function direct the architecture to start scrapy with "start_urls".</p>
<textarea style="color:red" rows="4" cols="80">
  def after_login(self, response) :
        for url in self.start_urls :
            yield self.make_requests_from_url(url)
</textarea>
<p>"Process_items.py" reads scrapied data from redis and write it into a file. </p>
<textarea style="color:red" rows="10" cols="80">
#@process_items.py
from __future__ import print_function, unicode_literals

import argparse
import json
import logging
import pprint
import sys
import time
from scrapy_redis import get_redis

logger = logging.getLogger('process_items')

def process_items(r, keys, timeout, limit=0, log_every=1000, wait=.1):
    with open('records.txt', 'w') as f:
      limit = limit or float('inf')
      processed = 0
      while processed < limit:
        # Change ``blpop`` to ``brpop`` to process as LIFO.
        ret = r.blpop(keys, timeout)
        # If data is found before the timeout then we consider we are done.
        if ret is None:
            time.sleep(wait)
            continue
        source, data = ret
        try:
        # The data is serialized into bytes that needs to be decoded into str for json to load
            data = data.decode()
            item = json.loads(data)
        except Exception:
            logger.exception("Failed to load item:\n%r", pprint.pformat(data))
            continue
        try:
            records = item.get('records')
            print(records)
            for i in records:
                f.write(i)
            f.write("**a post end mark**\n")
        except KeyError:
            logger.exception("[%s] Failed to process item:\n%r", source, pprint.pformat(item))
            continue
        processed += 1
        if processed % log_every == 0:
            logger.info("Processed %s items", processed)


def main():
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('key', help="Redis key where items are stored")
    parser.add_argument('--host')
    parser.add_argument('--port')
    parser.add_argument('--timeout', type=int, default=5)
    parser.add_argument('--limit', type=int, default=0)
    parser.add_argument('--progress-every', type=int, default=100)
    parser.add_argument('-v', '--verbose', action='store_true')

    args = parser.parse_args()

    params = {}
    if args.host:
        params['host'] = args.host
    if args.port:
        params['port'] = args.port

    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)

    r = get_redis(**params)
    host = r.connection_pool.get_connection('info').host
    logger.info("Waiting for items in '%s' (server: %s)", args.key, host)
    kwargs = {
        'keys': [args.key],
        'timeout': args.timeout,
        'limit': args.limit,
        'log_every': args.progress_every,
    }
    try:
        process_items(r, **kwargs)
        retcode = 0  # ok
    except KeyboardInterrupt:
        retcode = 0  # ok
    except Exception:
        logger.exception("Unhandled exception")
        retcode = 2

    return retcode

if __name__ == '__main__':
    sys.exit(main())
</textarea>
  
<p>"pipeline.py" adds more content to the scrapied data, e.g., spider name, crawled time.</p>
<textarea style="color:red" rows="10" cols="80">
# @pipeline.py
from datetime import datetime

class ZhihuPipeline(object):
    def process_item(self, item, spider):
        item["crawled"] = datetime.utcnow()
        item["spider"] = spider.name
        return item
</textarea>

<p>"items.py" defines the structure of scraped data to store in the redis.</p>
<textarea style="color:red" rows="10" cols="80">
# @items.py
from scrapy.item import Item, Field
class ZhihuItem(Item):
    records = Field()
    crawled = Field()
    spider = Field()
    url = Field()
</textarea>

<p>"settings.py" is the configuration file, the content is self-explained.</p>
<textarea style="color:red" rows="10" cols="80">
# @settings.py
SPIDER_MODULES = ['myproject.spiders']
NEWSPIDER_MODULE = 'myproject.spiders'

USER_AGENT = 'scrapy-redis (+https://github.com/rolando/scrapy-redis)'

DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
#SCHEDULER_PERSIST = True
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderPriorityQueue"
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderQueue"
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderStack"

SCHEDULER_IDLE_BEFORE_CLOSE = 10

ITEM_PIPELINES = {
    #'example.pipelines.ExamplePipeline': 300,
    #'scrapy_redis.pipelines.RedisPipeline': 400,
    'myproject.pipelines.ZhihuPipeline':300,
    'scrapy_redis.pipelines.RedisPipeline': 400,
}

#REDIS_HOST = 'localhost'
#REDIS_PORT = 6379

LOG_LEVEL = 'DEBUG'

# Introduce an artifical delay to make use of parallelism. to speed up the
# crawl.
DOWNLOAD_DELAY = 0.2

COOKIES_ENABLES=False

#disable default useragent and use a new useragent  
DOWNLOADER_MIDDLEWARES = {  
        'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware' : None,  
        'myproject.middlewares.RotateUserAgentMiddleware' : 400,
        #'myproject.middlewares.ProxyMiddleware': 100, #代理需要用到
    }

PROXIES = [
  {'ip_port': '111.11.228.75:80', 'user_pass': ''},
  {'ip_port': '120.198.243.22:80', 'user_pass': ''},
  {'ip_port': '111.8.60.9:8123', 'user_pass': ''},
  #{'ip_port': '101.71.27.120:80', 'user_pass': ''},
  {'ip_port': '122.96.59.104:80', 'user_pass': ''},
  {'ip_port': '122.224.249.122:8088', 'user_pass': ''},
  {'ip_port': '211.153.17.151:80', 'user_pass': ''},
  {'ip_port': '202.105.179.164:3128', 'user_pass': ''},
  {'ip_port': '202.107.238.51:3128', 'user_pass': ''},
  {'ip_port': '218.90.174.167:3128', 'user_pass': ''},
  {'ip_port': '14.29.124.52:80', 'user_pass': ''},
  {'ip_port': '112.112.70.116:80', 'user_pass': ''},
  {'ip_port': '61.132.241.103:808', 'user_pass': ''},
  {'ip_port': '123.125.122.203:80', 'user_pass': ''},
  {'ip_port': '123.125.122.205:80', 'user_pass': ''},
  {'ip_port': '123.125.122.224:80', 'user_pass': ''},
  {'ip_port': '123.125.122.204:80', 'user_pass': ''},
  {'ip_port': '115.28.101.22:3128', 'user_pass': ''},
  {'ip_port': '112.112.70.118:80', 'user_pass': ''},
  {'ip_port': '122.72.33.138:80', 'user_pass': ''},
  {'ip_port': '14.29.124.51:80', 'user_pass': ''},
  {'ip_port': '114.215.150.13:3128', 'user_pass': ''},
  {'ip_port': '113.107.112.210:8101', 'user_pass': ''},
  {'ip_port': '112.112.70.115:80', 'user_pass': ''},
  {'ip_port': '14.29.124.53:80', 'user_pass': ''},
  {'ip_port': '59.34.2.93:3128', 'user_pass': ''},
</textarea>

<h3>3 Anti-anti-crawl Methods</h3>
<p>添加随机user-agent</p>
<p>https://www.urlteam.org/2016/07/scrapy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%85%AB-%E5%8F%8D%E5%8F%8D%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/</p>
<p>登录</p>
<p>http://www.jianshu.com/p/b7f41df6202d</p>

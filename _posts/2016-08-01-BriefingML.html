---
layout: default
title: A Quick Look into Machine Learning Algorithms
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<p style="font-style:italic">The scientist’s discoveries impose his own order on chaos, as the composer or painter imposes his.
<p style="text-align: right; font-style:italic">— Arthur Koestler</p>
<br />
<p style="font-size:24px; font-weight:bold">I. Introduction</p>
<p style="text-align:center"><img src="/collections/imgs/1-ml-model.png" alt="basic ml model" style="width:400px;height:300px;" /></p>
<p style="text-align:center; font-style:italic">Fig. 1-1 A basic model of machine learning algorithm</p>

<p>The above figure depicts the basic model of machine learning and can be interpreted using a simple math formula:</p>
<p>$$ y = h(\sum_{i=0}^n \theta_i \times x_i) $$</p>
<p>\(x\) and \(y\) are the input and output data of the model, collected beforehand or to be prected; 
\(h\) represents "hypothesis" and is partially embodied by \(theta\) which are the parameters to learn. 
For any known machine learning problem, people follow this basic model no matter how complicated their specific algorithm is.</p>
<p>The essence of this models lies in two aspects: 1) using given set of \(x\) and \(y\) to discover a coherent relation or 
mapping between the input and the output, which is the learning process on \(theta\); 2) using learnt \(theta\) and newly input \(x\)
to predict \(y\).</p>
<p> If during the learning process, \(y\) is not provided, the algorithm proposed for such problem is called unsupervised learning
, or supervised learning otherwise. Notably, another type of learning algorithm called reinforcement learning gains increasingly 
more attention of the public. Basically, "Reinforcement learning differs from standard supervised learning in that correct input/output 
pairs are never presented, nor sub-optimal actions explicitly corrected.", as I quote from wikipedia.</p>

<p>We will discuss several supervised and unpervised learning algorithms in this post, but temporarily not dig into reinforcement learning (RL), thus insteand we present a few RL applications below to gain some quick intuitives.</p>
<p style="text-align:center"><img src="/collections/imgs/1-shinkansen-cn.png" style="width:400px;height:300px;" /><img src="/collections/imgs/1-shinkansen-jp.png" style="width:400px;height:300px;" /></p>
<p style="text-align:center; font-style:italic">Fig. 1-2 Two different shapes of bullet trains</p>
<p style="text-align:center"><img src="/collections/imgs/1-robot-walk.png" style="width:400px;height:300px;" /><img src="/collections/imgs/1-quadriplane-fly.png" style="width:400px;height:300px;" /></p>
<p style="text-align:center; font-style:italic">Fig. 1-3 Robot walking and quadriplane flying</p>
<br/>
<p style="font-size:24px; font-weight:bold">II. Regession</p>
<br />
<p style="font-size:18px; font-style:italic">1. Linear Regression</p>
<p>Linear regression is oftenly used to trian a "mapping" to predict the value of a CONTINUOUS variable (e.g., temperature, housing price, visitors, etc). Given a set of data with some inherent linear structure, we try fitting a line into the "sparse data". The data look likes "regressing" into a line which could be straight, curve, or more complicated shape, depends on the model choosed. The gist is to seek balance between under-fitting and over-fitting of data. </p>
<p style="text-align:center"><img src="/collections/imgs/2-linear-regression.png" style="width:400px;height:300px;" /><img src="/collections/imgs/2-linear-regression2.png" style="width:400px;height:300px;" /></p>
<p style="text-align:center; font-style:italic; font-weight:bold">Fig. 2-1 Two examples of linear regression</p>
<p>In Fig. 2-1, the first sub-figure simply follows the model:</p>
<p>$$ y = \sum_{i=0}^n \theta_i \times x_i $$</p>
<p>For the sub-figure in the right, higher order features were introduced:</p>
<p>$$ y = \sum_{i=0}^n \theta_i \times x_i + \theta_i' \times x_i^2 + ... $$</p>
<p>\(n\) represents the number of features, e.g., house area, location, floor, etc. for hourse price prediction. Introducing higher order of them may help improve the prediction accuracy, but as having said, needs to trade between under-fitting and over-fitting</p>
<p>To fit the line to data, we need a criterion on how well the line is fitted and thus we need a cost function:</p>
<p>$$J(\theta) = \frac{1}{2m}\sum_{j=0}^m (\sum_{i=0}^n \theta_i \times x_i^{(j)} - y_j)^2 $$</p>
<p>\(m\) is the number of training data size, that is, the number of (\(x\), \(y\)) pairs.<p>
<br />
<p style="font-size:18px; font-style:italic; font-weight:bold">2. Logistical Regression</p>
<p>Logistical regression would learn a model to classify data into two groups (e.g., spam email filter). To do this, we apply a logistical or sigmoid function on the linear regression model, which "squashes" y's domain from (-inf,inf) to (0,1). We benefit from two aspects: 1) we can interpret the value of y in (0,1) as a probability variable to construct a cost functoin; 2) the logistical function is derivative in (-inf, inf) to be applied with optimzation algorithm. Actually, sigmoid function is believed as the simplest example of a log-linear model which presents a dozen favorable attributes.</p>
<p>$$ y = \frac{1}{1+e^{\theta^{T}X}} $$</p>
<p>In above logistical model, \(\theta\) and \(X\) are vectors and \(T\) is transpose.</p>
<p style="text-align:center"><img src="/collections/imgs/2-sigmoid-function.png" style="width:400px;height:300px;" /></p>
<p style="text-align:center; font-style:italic; font-weight:bold">Fig. 2-2 Logistical/Sigmoild function</p>
<br />
<p>$$ J(\theta) = -\left[ \sum_{i=1}^m y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log (1-h_\theta(x^{(i)})) \right] $$</p>
<p>\(J(\theta)\) is the cost function for logistical regression. It's simply deduced from take log-function on the likehood formula of \(y\)'s probablity.</p>

<p style="font-size:18px; font-style:italic; font-weight:bold">3. Under-fitting & Over-fitting</p>
<p>Overfitting occurs when a statistical model or machine learning algorithm captures the noise of the data. Intuitively, overfitting occurs when the model or the algorithm fits the data too well. Specifically, overfitting occurs if the model or algorithm shows low bias but high variance.  Overfitting is often a result of an excessively complicated model, and it can be prevented by fitting multiple models and using validation or cross-validation to compare their predictive accuracies on test data.</p>
<p>Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data.  Intuitively, underfitting occurs when the model or the algorithm does not fit the data well enough.  Specifically, underfitting occurs if the model or algorithm shows low variance but high bias.  Underfitting is often a result of an excessively simple model.</p>
<p>We employ two terms bias and variance to judge whether a learnt model is overfitting or underfitting. <a href="http://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf">"The Bias-Variance Tradeoff"</a> provides a detailed explanation on how they are deduced.</p>
<p style="text-align:center"><img src="/collections/imgs/2-fitting-problem.png" style="width:800px;height:600px;" /></p>
<p style="text-align:center; font-style:italic; font-weight:bold">Fig. 2-2 Under-/Over-fitting</p>
<p>In practical projects, we usually plot a figure according to training error and cross validation to reflect the bias and vairance. Read this <a href="">post</a> for details.</p>
<br />
<p style="font-size:18px; font-style:italic; font-weight:bold">4. Regularization</p>
<p>Regularization allows complex models to be trained on data sets of limited size without severe ovefitting, by limiting the effective model complexity.</p>
<p>$$ J(\theta) = \frac{1}{2m}(\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})^2 + \lambda\sum_{i=1}^n\theta_j^2) $$</p>
<p>\(m\) is the size of training data and \(n\) is the number of the paramter \theta.</p>
<br />
<p style="font-size:18px; font-style:italic; font-weight:bold">5. Parameter Learning </p>
<p>The learning process is to find the set of parameters \(\theta\) that minimize the cost functin. The cost function for linear and logistical regression are convex, hence we can apply a bunch of optimization algorithms to find an optimal value. A well known algorithm is Newton's method, but it involves calcuating inverse matrix which is computation resource intensive when come to large number of parameters, so we simply apply gradient descent of which the gist is to reduce the cost toward its gradient's direction step by step, till a criterion is met:</p>
<p>$$ \theta = \theta - \alpha \times \nabla_\theta J(\theta) = \theta - \alpha \times \sum_{j=0}^m (\sum_{i=0}^n \theta_i \times x_i^{(j)} - y_j) \cdot x $$</p>
<p>We use linear regression as a example and the cost function is the Euclidean distance between \(h(\theta)\) and \(y\); \(\alpha\) is the learning rate and in more advanced algorithm it would be automatically modulated to learn faster.</p>
<br />
<p style="font-size:18px; font-style:italic; font-weight:bold">6. Softmax and Cross Entropy</p>
<p>Logistical regression classfies the output into two groups, and when there are more classes, we apply softmax regression.</p>
<p>$$
h_\theta(x) =
\begin{bmatrix}
P(y = 1 | x; \theta) \\
P(y = 2 | x; \theta) \\
\vdots \\
P(y = K | x; \theta)
\end{bmatrix}
=
\frac{1}{ \sum_{j=1}^{K}{\exp(\theta^{(j)\top} x) }}
\begin{bmatrix}
\exp(\theta^{(1)\top} x ) \\
\exp(\theta^{(2)\top} x ) \\
\vdots \\
\exp(\theta^{(K)\top} x ) \\
\end{bmatrix}
$$</p>
<p>Here \(\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(K)} \in \Re^{n}\) are the parameters of our model. Notice that the term \(\frac{1}{ \sum_{j=1}^{K}{\exp(\theta^{(j)\top} x) } }\) normalizes the distribution, so that it sums to one.</p>
<p>$$
J(\theta) = - \left[ \sum_{i=1}^{m} \sum_{k=1}^{K}  1\left\{y^{(i)} = k\right\} \log \frac{\exp(\theta^{(k)\top} x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top} x^{(i)})}\right]
$$</p>
<p>The cost function of softmax regression is usually called cross entropy, which is an idea from information theory.</p>
<p>Notice that this generalizes the logistic regression cost function, which could also have been written:</p>
<p>$$
J(\theta) &= - \left[ \sum_{i=1}^m   (1-y^{(i)}) \log (1-h_\theta(x^{(i)})) + y^{(i)} \log h_\theta(x^{(i)}) \right] \\
&= - \left[ \sum_{i=1}^{m} \sum_{k=0}^{1} 1\left\{y^{(i)} = k\right\} \log P(y^{(i)} = k | x^{(i)} ; \theta) \right]
$$</p>
<p>To train data with gradient descent method, firstly need to find the form of gradient of \(J(\theta)\):</p>
<p>$$
\nabla_{\theta^{(k)}} J(\theta) = - \sum_{i=1}^{m}{ \left[ x^{(i)} \left( 1\{ y^{(i)} = k\}  - P(y^{(i)} = k | x^{(i)}; \theta) \right) \right]  }
$$</p>
<p>Softmax regression is hugely useful, in deep nerual network, the last layer would usually adopt softmax regression to finally classify an object.</p>

<br />
<p style="font-size:24px; font-weight:bold">III. K-Mean and SVM</p>

<br />
<p style="font-size:24px; font-weight:bold">IV. Baysian Model and Markov Process</p>

<br />
<p style="font-size:24px; font-weight:bold">V. Hidden Markov Process and EM Algorithm</p>

<br />
<p style="font-size:24px; font-weight:bold">VI. Convolutional Nerual Network</p>

<br />
<p style="font-size:24px; font-weight:bold">VII. Recurrent Nerual Network</p>

<br />
<p style="font-size:24px; font-weight:bold">VIII. LSTM, GRU and Seq2seq Model</p>
<br />
<p style="font-size:18px; font-style:italic; font-weight:bold">1 LSTM and GRU Cell</p>
<br />
<p style="font-size:18px; font-style:italic; font-weight:bold">2 Seq2seq Model</p>

<br />
<p style="font-size:24px; font-weight:bold">IX. Dynamic Memory Network</p>

<br />
<p style="font-size:24px; font-weight:bold">X. Chatterbot</p>
<br />
<p style="font-size:18px; font-style:italic; font-weight:bold">1 Tradiation Language Modelling</p>
<br />
<p style="font-size:18px; font-style:italic; font-weight:bold">2 Nerual Language Modelling</p>
<br />
<p style="font-size:18px; font-style:italic; font-weight:bold">3 Word Embedding</p>
<br />
<p style="font-size:18px; font-style:italic; font-weight:bold">4 Neural Converation Model</p>


<br />
<p style="font-size:24px; font-weight:bold">XI. Scrapy Redis</p>

<br />
<p style="font-size:24px; font-weight:bold">XII. Tensorflow, Theano and Caffe</p>

<br />
<p style="font-size:24px; font-weight:bold">XIII. Linear and Convex Optimzation</p>

<br />
<p style="font-size:24px; font-weight:bold">Appendix I. References</p>

<br />
<p style="font-size:24px; font-weight:bold">Appendix II. Resources</p>

---
layout: default
title: Logistical Regression
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<h2>{{ page.title }}</h2>
<p>--BACKGROUND--</p>
<p>Unlike linear regression which predicts value in a continous space, logistical regression learns to classify data into two groups. 
  To do this, the linear function of parameters \(\theta\) and collected data \(x\) are substituted into a logistical or sigmoid function
  which "squashes" the value domain of \(y\)from (-inf,inf) to (0,1). In this case, we can interpret y in (0,1) as a probability to 
  construct a cost functoin. For many machine learning models, the idea is to firstly express a "prediction" data \(y^~\) in the form of 
  probabilities (i.e., combination of all possible outcomes with corresspoinding probability). Then the distance between \(y^~\) and
  \(y\) (called label) is deemed as model cost, minizing of which is to tune the parameters of the model to improve the prediction accuracy.
  Note that the logistical function is derivative in (-inf, inf) and we can apply gradient descent to find the solution. </p>
<p>--MODEL--</p>
<p><m>$$ y = h(\theta) = \frac{1}{1+e^{-\theta^T X}} $$</m></p>
<p>For logistical function, its derivative has a property:</p>
$$
\begin{align}
g'(z) & = \frac{d}{dz} \frac{1}{1+e^{-z}}\\
      & = (\frac{1}{1+e^{-z}})^2 e^{-z}\\
      & = \frac{1}{1+e^{-z}} (1 - \frac{1}{1+e^{-z}})\\
      & = g(z)(1-g(z))
\end{align}
$$
<p>The above model assumes y is a linear combination of x. If the model fail to fit the data well, you may consider introduce higher order variable(s) which might be like:</p>
<p>$$ y = \sum_{i=0}^n \theta_i \times x_i + \theta_i' \times x_i^2 + ... $$</p>
<p>In later posts, we'll discuss how to do model selection.</p>
<p>--COST FUNCTION--</p>
<p>$$J(\theta) = 1/2 \times \sum_{j=0}^m (\sum_{i=0}^n \theta_i \times x_i^{(j)} - y_j)^2 $$</p>
<p>The cost function measures how y is fit into x. Minimizing the cost we would learn an optimal parameter value (set). The minimizing process is to solve a convex optimization problem for quadratic function. We can use gradient descent or Newton's method to solve it. We apply gradient descent here and we need to run below formula iteratively until convergence.</p>
<p>$$\theta = \theta - \alpha \times \nabla_\theta J(\theta) = \theta - \alpha \times \sum_{j=0}^m (\sum_{i=0}^n \theta_i \times x_i^{(j)} - y_j) \cdot x $$</p>
<p>--EXAMPLE--</p>
<p>We use the exercise of stanford's deep learning course on linear regression as example to illustrate how it works. The exercise is originally implemented using matlab, but here we use python.</p>

<p>--More--</p>
<p>In this post, we focus on introducing the model and provide an example of how to do linear regression with python. We will discuss newton's method, close-form of the optimal value,  higher order model, over/under-fitting, etc, in later posts.</p>

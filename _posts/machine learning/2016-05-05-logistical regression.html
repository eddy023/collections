---
layout: default
title: logistical regression
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<h2>{{ page.title }}</h2>
<p>--BACKGROUND--</p>
<p>Linear regression is used to learn a model to classify data into two groups. Actually, we apply a logistical or sigmoid function on the simple model of linear regression, which "squashes" y's domain from (-inf,inf) to (0,1). This came out of two reason: 1) we can interpret y in (0,1) as a probability to construct a cost functoin; 2) the logistical function is derivative in (-inf, inf). Actually, there is another important and more formal reason why apply logistical function we will discuss in a later post.</p>
<p>--MODEL--</p>
<p><m>$$ y = \sum_{i=0}^n \theta_i \times x_i $$</m></p>
<p>The above model (hypothesis) assumes y is a linear combination of x. If the model fail to fit the data well, you may consider introduce higher order variable(s) which might be like:</p>
<p>$$ y = \sum_{i=0}^n \theta_i \times x_i + \theta_i' \times x_i^2 + ... $$</p>
<p>In later posts, we'll discuss how to do model selection.</p>
<p>--COST FUNCTION--</p>
<p>$$J(\theta) = 1/2 \times \sum_{j=0}^m (\sum_{i=0}^n \theta_i \times x_i^{(j)} - y_j)^2 $$</p>
<p>The cost function measures how y is fit into x. Minimizing the cost we would learn an optimal parameter value (set). The minimizing process is to solve a convex optimization problem for quadratic function. We can use gradient descent or Newton's method to solve it. We apply gradient descent here and we need to run below formula iteratively until convergence.</p>
<p>$$\theta = \theta - \alpha \times \nabla_\theta J(\theta) = \theta - \alpha \times \sum_{j=0}^m (\sum_{i=0}^n \theta_i \times x_i^{(j)} - y_j) \cdot x $$</p>
<p>--EXAMPLE--</p>
<p>We use the exercise of stanford's deep learning course on linear regression as example to illustrate how it works. The exercise is originally implemented using matlab, but here we use python.</p>

<p>--More--</p>
<p>In this post, we focus on introducing the model and provide an example of how to do linear regression with python. We will discuss newton's method, close-form of the optimal value,  higher order model, over/under-fitting, etc, in later posts.</p>

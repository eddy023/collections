---
layout: default
title: Dynamic Memory Network
---

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h3>1. The model</h3>
<p>The Dynamic Memory Network(DMN) consists of 4 sub-modules: input, question, episodic memory and answer (Fig. 1-1). 
  Before sequencing words of a sentence into the input model, they are transformed into 
  <a href="https://eddy023.github.io/collections/2016/08/26/WordEmbedding.html">word embeddings</a> 
  beforehand. Per sentence, the input model will generate a vector representation which is called fact. 
  All facts will be stored in the episodic memory and get "scanned" through to spot relevent information to given question, 
  using a called "attention mechanism". The scan may take several passes and each pass an "episode" is produced. All episodes
  are finally "combined" into a answer.</p>
<p style="text-align:center"><img src="/collections/imgs/DMN-1-Model.png" style="width:900px;height:400px;" /></p>
<p style="text-align:center; font-style:italic; font-weight:bold">Fig. 1-1 Structure of dynamic memory network</p>

<h3>2. How DMN works</h3>
<p style="text-align:center"><img src="/collections/imgs/DMN-1-Example.png" style="width:900px;height:400px;" /></p>
<p style="text-align:center; font-style:italic; font-weight:bold">Fig. 2-1 An example of workflow of dynamic memory network</p>
<br>
<p>Fig. 2-1 demonstrates how the dynaminc memory network works to answer a question in 4 steps. </p>
<br>
<p>(1) at the beginning, transform input sentences into word embeddings, e.g., using Glove [1] 
  (also see <a href="https://eddy023.github.io/collections/2016/08/26/WordEmbedding.html">word embeddings</a>). 
  Word embeddings is then passed to a gated recurrent network (GRU) to generate sentence representations. More specifically, 
  given sets of word embeddings of sentences (e.g., \(s_1\), \(s_2\), ..., \(s_8\); \(s_i\) is a list of word embeddings 
  with each embedding corresponding to a word in original sentence), a "End of Sentence" token is added at the end of each 
  sentence. These sentences are inputted into the GRU a word per iteration. After serveral iterations when the input word 
  is a token, the hidden state of the GRU is deemed as the sentence　representation, and the output of the GRU is recorded 
  as a fact \(c_1\). In this patter, the input model generates \(c_2\), \(c_3\), ..., \(c_T\).　In math, above procedures 
  can be summaried as:
  $$ C_t = GRU(L[\omega_t^I], h_{t-1}) $$ 
  \(L\) is the embedding matrix. Both GRU or LSTM can be used, since LSTM performs similarly to GRU yet more computational 
  expensive, GRU is preferred. </p>
<p>(2) the question module employs a GRU to generate a question representation as well:
  $$ q_t = GRU(L[w_t^Q], q_{t-1}) $$ 
  $$ q = q_{T_Q} $$ </p>
<p>(3) in the episodic memory module, attention mechanism is applied to generate episode \(e\). But a slightly different
  gated network is used as shown in Fig. 2-2. </p>
  <p style="text-align:center"><img src="/collections/imgs/DMN-weight-g.png" style="width:800px;height:300px;" /></p>
  <p style="text-align:center; font-style:italic; font-weight:bold">Fig. 2-2 Original GRU v.s. Modified GRU</p>
  <br>
  <p> The Update gate is replaced by a new gate operation \(g\) and the formula to calculate \(e\) becomes:
  $$ h^i_t = g^i_t \cdot \sim:h_t + (1 - g^i_t) \cdot h^i_{t-1}$$
  $$ e^i = h^t_{T_c} $$
  \(h^i_t\) is the hidden state of the GRU; the update gate \(g^i_t\) is used as weight and calculated as:
  $$
  \begin{align}
  g^i_t &= G(c_t, m^{i-1}, q) \\
        &= \sigma(w^{(2)}tanh(w^{(1)}z(c_t, m^{i-q}, q) + b^{(1)}) + b^{(2)})
  \end{align}
  $$
  \(G\) is in fact a two-layer feed forward network. The \(\sigma\) operation is actually a softmax operation. 
    \(z(c, m, q)\) is the feature vector which consists of a variety of similarities:
  $$ z(c, m, q) = [ c, m, q, c ◦ q, c ◦ m, |c − q|, |c − m|, c^T W^{(b)} q, c^T W^{(b)} m ] $$
  For each pass, the attention mechanism performs \(T_c\) iterations of RNN (per on each input fact) and lastly outputs 
  \(e^{i}\). \(e^{i}\) then participates in caculcating \(m^i\) which is intiated by q:
  $$ m^i = GRU(e^i, m^{i-1}) $$
  $$ m^0 = q $$ 
  \(m^i\) again helps to generate \(e^{i+1}\). In this way, facts are reasoned in multiple pass till find the answer. 
</p>
<p>(4) \(e^i\) is passed to the answer module.
</p>
<h3>3. Speculation</h3>

<h3>4. References</h3>
<p>[1] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP, 2014.</p>
